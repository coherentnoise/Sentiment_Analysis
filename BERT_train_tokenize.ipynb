{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afcddfe",
   "metadata": {},
   "source": [
    "# Training a masked BERT model\n",
    "\n",
    "## Performs tokenization and training\n",
    "\n",
    "### This is the first step before fine-tuning and downstream tasks such as zero-shot classification and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fc7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertForMaskedLM\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig(\n\u001b[1;32m      4\u001b[0m     hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m384\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      6\u001b[0m     num_hidden_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_attention_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      8\u001b[0m     intermediate_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForMaskedLM(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mnum_parameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "config = BertConfig(\n",
    "    hidden_size = 384,\n",
    "    vocab_size= tokenizer.vocab_size,\n",
    "    num_hidden_layers = 6,\n",
    "    num_attention_heads = 6,\n",
    "    intermediate_size = 1024,\n",
    "    max_position_embeddings = 256\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config=config)\n",
    "print(model.num_parameters()) #10457864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a10570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457cffd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c45e5098544b14a94ad187a4556417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a56cfde5dc4326b5e589557995a491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e01de69088c4c64b936a62d7cefadfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c448d67901442b289958ae5429b45a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tokenizer_base \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# convert pandas dataset to HF dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# define iterator\u001b[39;00m\n\u001b[1;32m     13\u001b[0m training_corpus \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     dataset[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1000\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset), \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "#load base tokenizer to train on dataset\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# convert pandas dataset to HF dataset\n",
    "dataset = Dataset.from_pandas(df.rename(columns={\"comment\":'text'}))\n",
    "\n",
    "# define iterator\n",
    "training_corpus = (\n",
    "    dataset[i : i + 1000][\"text\"]\n",
    "    for i in range(0, len(dataset), 1000)\n",
    ")\n",
    "\n",
    "\n",
    "#train the new tokenizer for dataset\n",
    "tokenizer = tokenizer_base.train_new_from_iterator(training_corpus, 5000)\n",
    "#test trained tokenizer for sample text\n",
    "text = dataset['text'][123]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c9b98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's check tokenization process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      3\u001b[0m subword_view \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[38;5;28mid\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m input_ids]\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39marray(subword_view)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# let's check tokenization process\n",
    "input_ids = tokenizer(text).input_ids\n",
    "subword_view = [tokenizer.convert_ids_to_tokens(id) for id in input_ids]\n",
    "np.array(subword_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained tokenizer\n",
    "\n",
    "tokenizer.save_pretrained(\"tokenizer/sinhala-wordpiece-yt-comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99185f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator and tokenize dataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47490391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "class LineByLineTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, raw_datasets, max_length: int):\n",
    "        self.padding = \"max_length\"\n",
    "        self.text_column_name = 'text'\n",
    "        self.max_length = max_length\n",
    "        self.accelerator = Accelerator(gradient_accumulation_steps=1)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with self.accelerator.main_process_first():\n",
    "            self.tokenized_datasets = raw_datasets.map(\n",
    "                self.tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=[self.text_column_name],\n",
    "                desc=\"Running tokenizer on dataset line_by_line\",\n",
    "            )\n",
    "            self.tokenized_datasets.set_format('torch',columns=['input_ids'],dtype=torch.long)\n",
    "            \n",
    "    def tokenize_function(self,examples):\n",
    "        examples[self.text_column_name] = [\n",
    "            line for line in examples[self.text_column_name] if len(line[0]) > 0 and not line[0].isspace()\n",
    "        ]\n",
    "        return self.tokenizer(\n",
    "            examples[self.text_column_name],\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_datasets)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokenized_datasets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = LineByLineTextDataset(\n",
    "    tokenizer= tokenizer,\n",
    "    raw_datasets = dataset,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"Ransaka/sinhala-bert-yt-comments\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=5_000,\n",
    "    logging_steps = 1000,\n",
    "    save_total_limit=2,\n",
    "    use_mps_device = True, # disable this if you're running non-mac env\n",
    "    hub_private_repo = False, # please set true if you want to save model privetly\n",
    "    save_safetensors= True,\n",
    "    learning_rate = 1e-4,\n",
    "    report_to='wandb'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset_train\n",
    ")trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c742cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the trainer\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can now be used for downstream tasks such as zero-shot classification and clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
